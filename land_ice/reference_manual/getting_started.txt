MPAS Land Ice Core Quick Start:

============
Organization:
In this document, the following directory convention is used:
~/mpas   = a root directory to hold all mpas related work
~/mpas/land_ice_projects   = https://svn-mpas-model.cgd.ucar.edu/branches/land_ice_projects  ; where the land ice development happens
~/mpas/grid_gen   = https://svn-mpas-model.cgd.ucar.edu/trunk/grid_gen  ; the trunk version of grid generation tools - this is the only piece of the trunk that is currently used
~/mpas/grids   = an unversioned directory in which to generate grids
~/mpas/runs   = an unversioned directory in which to actually perform runs
(Note: the land_ice_projects and grid_gen directories can either be separate checkouts or symlinks to a single checkout of the entire repository, e.g. ~/mpas/mpasall/.  The latter option allows a single 'snv up' to update all the pieces.)
The quick start assumes you want to run the dome test case.

================================
Checking out the land ice branch
================================
$ svn checkout --username somebody@lanl.gov https://svn-mpas-model.cgd.ucar.edu/branches/land_ice_projects ~/mpas/land_ice_projects

All land ice development occurs within the land_ice_projects branch in separate projects.
The land ice core is located in the 'implement_core' project which is equivalent to the 'mpas' directory in the trunk.

Updating the branch (if already checked out)
$ svn update


========================
Setting up a workspace
========================
We will create unversioned directories called 'grids' and 'runs' where simulations are performed.  All work can be done in the directories under version control, but this is somewhat cleaner for illustrative purposes.
mkdir ~/mpas/grids
mkdir ~/mpas/grids/dome
mkdir ~/mpas/runs
cp -R ~/mpas/land_ice_projects/test_cases/dome ~/mpas/runs/dome
(this could also be a symlink)


=======================
Building MPAS: Land Ice
=======================
$ setenv CORE land_ice (or export CORE=land_ice)
$ setenv NETCDF /usr/local/netcdf-gfortran (or export NETCDF=/usr/local/netcdf-gfortran)

$ cd ~/mpas/land_ice_projects/implement_core
$ make clean
$ make gfortran DEBUG=true 
Iif you do not have MPI installed and working properly, you can compile with "make gfortran SERIAL=true".  However the option to do a serial build will soon be deprecated.)
(You can replace gfortran with the compiler of your choice - see Makefile for options)

look for the link to land_ice_model.exe to make sure the build succeeded

Notes on building:
* Starting with netcdf4, linking to the netcdf fortran libraries requires both netcdf.a and netcdff.a.  The Makefile needs to be modified to include -lnetcdff on the LIBS line.  (Same for periodic_hex's Makefile.)  If you are using netcdf version 3.x, this doesn not apply.  NOTE: The MPAS Makefile has now been modified to automatically detect if you are using netcdf4 and make the change for you.
* On bluefire 'make' will not work - gmake needs to be used instead.  One way to avoid having to replace all instances of 'make' with 'gmake' in all Makefiles is to set up a link named 'make' to /usr/local/bin/gmake in your home directory.  For example:
cd ~
mkdir bin
ln -s /usr/local/bin/gmake ./bin/make
Now put /glade/home/USERNAME/bin at the beginning of your path in your .bashrc by adding this line:
export PATH=/glade/home/USERNAME/bin:$PATH
Just remember that from now on you will always be using gmake when you invoke 'make'!


=============================
Building with and using LifeV
=============================
If you want to compile with the LifeV velocity solvers, you will need to have LifeV compiled and tested successfully.  LifeV requires Trilinos (10.10.1), ParMetis (3.2), HDF5 (1.8.8).  Parentheses indicate versions that I was successful with - not all versions (and not necessarily the newest) of each will work.

To compile MPAS with LifeV libraries use:
$ make gfortran DEBUG=true LIFEV=true
but you first need to define an environment variable called EXTERNAL_LIBS that specifies all of the needed LifeV libraries.  You should be able to obtain the needed information from the Makefile.export.LifeV_install file in your LifeV build directory.  You will need the information from LifeV_LIBRARY_DIRS, LifeV_LIBRARIES, LifeV_TPL_LIBRARY_DIRS, LifeV_TPL_LIBRARIES.  You may also need: -lsz -lmpi_cxx -lmpi_f90 -lstdc++
For example, I use:
export TRILINOS=/Users/mhoffman/software/trilinos/trilinos-10.10.1-GCC-MPI-OPT
export LIFEV=/Users/mhoffman/software/lifev/lifev-build/lifev
export EXTERNAL_LIBS="-L$LIFEV/ice_sheet/ -llifevicesheetinterface -L$LIFEV/core -llifevcore \
-L/$TRILINOS/lib \
-lpiro -lmoochothyra -lmoocho -lrythmos -llocathyra -llocaepetra -llocalapack -lloca -lnoxthyra -lnoxepetra -lnoxlapack -lnox -lanasazitpetra -lModeLaplace -lanasaziepetra -lanasazi -lstratimikos -lstratimikosbelos -lstratimikosaztecoo -lstratimikosamesos -lstratimikosml -lstratimikosifpack -lbelostpetra -lbelosepetra -lbelos -lml -lifpack -lamesos -lgaleri -laztecoo -lisorropia -loptipack -lthyratpetra -lthyraepetraext -lthyraepetra -lthyracore -lepetraext -ltpetraext -ltpetrainout -ltpetra -ltriutils -lglobipack -lzoltan -lepetra -lkokkoslinalg -lkokkosnodeapi -lkokkos -lrtop -lsacado -ltpi -lteuchos \
-lpiro -lmoochothyra -lmoocho -lrythmos -lmoertel -llocathyra -llocaepetra -llocalapack -lloca -lnoxthyra -lnoxepetra -lnoxlapack -lnox -lintrepid -lanasazitpetra -lModeLaplace -lanasaziepetra -lanasazi -lfei_trilinos -lfei_base -lstratimikos -lstratimikosbelos -lstratimikosaztecoo -lstratimikosamesos -lstratimikosml -lstratimikosifpack -lifpack2 -lbelostpetra -lbelosepetra -lbelos -lml -lkomplex -lifpack -lpamgen_extras -lpamgen -lamesos -lgaleri -laztecoo -ldpliris -lisorropia -loptipack -lthyratpetra -lthyraepetraext -lthyraepetra -lthyracore -lthyratpetra -lthyraepetraext -lthyraepetra -lthyracore -lepetraext -ltpetraext -ltpetrainout -ltpetra -ltriutils -lglobipack -lshards -lzoltan -lepetra -lkokkoslinalg -lkokkosnodeapi -lkokkos -lrtop -lsacado -ltpi -lteuchos \
/Users/mhoffman/software/hdf/hdf5-1.8.8-mpif90-gcc4.5/lib/libhdf5.a /Users/mhoffman/software/hdf/hdf5-1.8.8-mpif90-gcc4.5/lib/libhdf5_hl.a /usr/lib/libz.dylib /opt/local/lib/libz.dylib /opt/local/lib/libnetcdf.dylib /Users/mhoffman/software/parmetis/ParMetis-3.1.1/libparmetis.a /Users/mhoffman/software/parmetis/ParMetis-3.1.1/libmetis.a /usr/lib/liblapack.dylib /usr/lib/libblas.dylib /usr/lib/libpthread.dylib /Users/mhoffman/software/parmetis/ParMetis-3.1.1/libparmetis.a /Users/mhoffman/software/parmetis/ParMetis-3.1.1/libmetis.a /usr/lib/liblapack.dylib /usr/lib/libblas.dylib \
-L/opt/local/lib -lsz \
-lmpi_cxx -lmpi_f90 -lstdc++"


To actually perform calls to LifeV, your namelist.config file needs to specify either 'L1L2', 'FO', or 'Stokes' for the config_dycore option.  You can still run the model without calling LifeV if you've compiled with LifeV (e.g. set config_dycore = 'SIA' to use the MPAS-based SIA velocity solver).  If you modify your LifeV libraries, MPAS' Makefile won't know about it - you'll need to 'make clean; make ...', or, faster, 'rm src/land_ice_model.exe; make ...'.


=============
Making a grid
=============
Checking out the trunk version of grid gen
$ svn checkout --username somebody@lanl.gov https://svn-mpas-model.cgd.ucar.edu/trunk/grid_gen ~/mpas/grid_gen

$ cd ~/mpas/grid_gen/periodic_hex
$ vim Makefile
Modify the Makefile to use your compiler (this one is not smart like the Makefile in mpas/).
For example:
FC = gfortran
CC = gcc
FFLAGS = -O3 -m64 -ffree-line-length-none -fdefault-real-8 -fconvert=big-endian
CFLAGS = -O3 -m64
LDFLAGS = -O3 -m64

$ make


Now we will work from our 'grids' directory to actually create the grid:
$ cd ~/mpas/grids/dome/
Get the namelist file that will create the appropriately sized grid for the dome test case
$ cp ~/mpas/land_ice_projects/test_cases/dome/namelist.input.periodic_hex ./namelist.input
$ ln -s ~/mpas/grid_gen/periodic_hex/periodic_grid 

If you'd like, modify the namelist file:
$ vim namelist.input (or emacs namelist.input)
&periodic_grid
   nx = 30, # the number of columns
   ny = 35, #                     ...rows
   dc = 2000., # the characteristic size of a cell
   nVertLevels = 9, # the number of vertical levels in the grid
   nTracers = 1, # the number of tracers - this is not used by the land ice core (gets removed in a subsequent step), but periodic_hex requires a value
   nproc = 2, 4, 8, # one entry each for the number of processors that should be supported for parallel runs, generates data partitioning in files graph.info.part.*
/

Create the grid:
$ ./periodic_grid
Look for:
grid.nc
graph.info.part.2
graph.info.part.4
etc.


Next the default MPAS grid needs to be modified to include land ice variables.  This is currently done with a python script.
The script is located here:  land_ice_projects/grid_tools/add_land_ice_variables_to_mpas_grid.py
Run the script with:
python ~/mpas/land_ice_projects/grid_tools/add_land_ice_variables_to_mpas_grid.py
(you can also copy it to the current directory and then run it)
It will create a new file called land_ice_grid.nc that contains the land ice variables.  (Note that the script will use grid.nc as the input file by default, but a different file name can be passed in as an argument.)
You can confirm this worked with ncdump -h land_ice_grid.nc.  At the end of the variable list should be:
        double layerThicknessFractions(nVertLevels) ;
        double thickness(Time, nCells) ;
        double bedTopography(Time, nCells) ;
        double beta(Time, nCells) ;
        double normalVelocity(Time, nEdges, nVertLevels) ;
        double temperature(Time, nCells, nVertLevelsPlus2) ;



================
Setting up a Run:
================
The following assumes the following locations:
land_ice branch is in ~/mpas/land_ice_projects   (this should include MPAS as the 'implement_core' project)
grids are stored in subdirectories in ~/mpas/grids
runs are made from subdirectories in ~/mpas/runs

Move to the directory for running the code and gather needed files
$ cd ~/mpas/runs/dome
$ ln -s ~/mpas/land_ice_projects/implement_core/land_ice_model.exe 
$ cp namelist.input.land_ice_core namelist.input
There should be no need to modify namelist.input as it is setup to run the dome test case.
$ cp ~/mpas/grids/dome/land_ice_grid.nc .
(The grid could also be symlinked if it is large, but in this case it is small.)

Now the dome initial conditions need to be added to the grid file.  This is currently done with a python script:
python setup_dome_initial_conditions.py
(By default this script modifies land_ice_grid.nc, but a different file name can be specified as a command line argument.)
You can check that the script worked with:
ncdump -v thickness land_ice_grid.nc
You should see a bunch of nonzero values for the thickness.

if running on 1 processor:
$ ./land_ice_model.exe

if running on 2 processors:
$ ln -s ~/mpas/grids/dome/graph.info.part.2 graph.info.part.2
$ mpirun -np 2 land_ice_model.exe



Output:
 Reading namelist.input
  
 Initial timestep 0000-01-01_00:00:00             
 Doing timestep 0000-01-01_00:01:00             
   timer_name                                  total     calls          min            max            avg     percent  efficiency
 0 total time                                 0.43400         1        0.43400        0.43400        0.43400
 1  initialize                                0.03700         1        0.03700        0.03700        0.03700    0.09    0.00
 1  time integration                          0.00000         1        0.00000        0.00000        0.00000    0.00    0.00


Now look for output.nc

The default velocity solver is a SIA solver written in MPAS.  To use a higher-order solver, you need to have MPAS compiled with LifeV support (see above).


========================
Important Config Options
========================
Runs in MPAS are controlled by options set in the namelist.config file in the directory where you execute the run.  Any options not specified in the namelist.config file are determined from defaults set in src/core_land_ice/Registry.  Browse that file to see what valid options exist (though not all are implemented yet).  Here are the more important options:
   config_time_integration = 'ForwardEuler'  ! This is the only currently supported time integration scheme
   config_dycore = 'FO'   ! This is where you select the velocity solver.  Valid dycores are SIA, L1L2, FO, Stokes. All but SIA require compiling with LifeV libraries.
! Specify run duration with these.  MPAS uses a calendar system, so start/stop times need to be specified in this format.  You can alternatively specify a config_run_duration, but it would have to be in seconds.  It is possible to specify dt in seconds instead of years with config_dt_seconds.
   config_dt_years = 1.0
   config_start_time = '0000-01-01_00:00:00'
   config_stop_time = '0005-01-01_00:00:00'
! I/O specified with these:
   config_input_name = 'land_ice_grid.nc'
   config_output_name = 'output.nc'
   config_restart_name = 'restart.nc'


================
Examining Output
================
There are a number of options for visualizing output:

1. Some simple plots of the dome test case are generated with a python script:
python visualize_dome.py
It can take commandline arguments to visualize a different time step (default=the first) or file (default=output.nc) and to save the plots to files.  Get details with: python visualize_dome.py -h

2. Use paraview to visualize output.nc (this will have to be a separate tutorial).
However, paraview's MPAS support is currently limited to variables with dimensions that include [nCells (or nVertices), nVertLevels], i.e. 3d fields only.  Few land ice variables meet that criterion.  Hopefully this will be remedied in a future version of the paraview MPAS plugin.

3. An alternative is to convert the output file to a regular grid that can be viewed (somwhat sloppily) in ncview (or similar tool).  A script that does that conversion is here:
land_ice_projects/grid_tools/convert_mpas_grid_to_regular_grid_netcdf.py

4. By far the best option for casually browsing output is a tool written by Doug Jacobsen called MpasDraw.  It is located here: branches/tools/mpas_draw, so you will need to checkout that directory if you have not checked out the entire repository.  To compile it, simply make sure the Makefile has properly set the NETCDF and PLATFORM variables, and then run 'make'.  The README file has detailed instructions for how to use it.

5. For more rigorous examination/analysis of the output, python or Matlab can be used.  


